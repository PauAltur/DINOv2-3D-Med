# This version of the YAML uses processed patches
project: "$@PROJECT_ROOT if @PROJECT_ROOT is not None else '.'"
run_name: "DINOv2_pretrain_primus"
output_dir: "$@OUTPUT_DIR if @OUTPUT_DIR is not None else './outputs'"
img_size: [160, 160, 160]
hidden_size: 864
patch_size: [8, 8, 8]

trainer:
  _target_: pytorch_lightning.Trainer
  benchmark: True
  max_epochs: 1000
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  enable_model_summary: False
  log_every_n_steps: 10
  limit_train_batches: 250
  fast_dev_run: False
  num_sanity_val_steps: 0
  precision: 16-mixed
  devices: 3
  detect_anomaly: False
  sync_batchnorm: True
  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: "dinov2"
    name: "@run_name"
    save_dir: '$"@output_dir/@run_name"'
  callbacks:
    - _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: '$"@output_dir/@run_name"'
      filename: "dinov2_3d_fixed_{epoch:03d}"
      save_last: True
      every_n_epochs: 1
      save_on_train_epoch_end: True

lightning_module:
    _target_: project.training.DINOtxt_LightningModule
    batch_size_per_device: 2
    vision_encoder: 
      _target_: project.models.backbones.vision_enc_wrapper.VisionEncoder_w_Blocks
      backbone:
        _target_: project.models.backbones.Primus
        embed_dim: "%hidden_size"
        eva_depth: 16
        eva_numheads: 12
        input_channels: 1
        num_classes: 1
        input_shape: "%img_size"
        patch_embed_size: "%patch_size"
        patch_drop_rate: 0.0
        classification: True
    text_encoder:
      _target_: project.models.backbones.text_encoder.TextEncoder
      model_name: "openai/clip-vit-base-patch32"
      output_dim: "%hidden_size"
      freeze_encoder: True
      use_projection: True
      pooling_strategy: "cls"
      max_length: 77

    vision_backbone:
        _target_: project.models.backbones.vision_enc_wrapper.create_enhanced_vision_encoder
        backbone:
            _target_: timm.models.vision_transformer.vit_base_patch16_224
            img_size: 224
            patch_size: 16
            embed_dim: 768
    text
    batch_size_per_device: 16
    hidden_size: "%hidden_size"
    ibot_separate_head: True
    base_lr: 0.0001
    layer_decay: 0.9
    gradient_clip_val: 3.0
    teacher_temp_warmup_epochs: 30
    teacher_temp_min: 0.005
    teacher_temp_max: 0.01
    freeze_last_layer_epochs: 1
    projection_dim: 16384
    weight_decay: 0.04
    backbone:

data_module:
  _target_: project.training.DataModule
  num_workers: 12
  batch_size: 2
  pin_memory: True
  drop_last: True
  train_dataset:
    _target_: project.utils.safe_dataset.SafeDataset # Dataset that reshuffles erroneous data! Use carefully as it might skip data 
    dataset:
      _target_: monai.data.Dataset
      data: 
      transform:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: monai.transforms.LoadImaged
              keys: ["image"]
              image_only: True
            - _target_: monai.transforms.EnsureChannelFirstd
              keys: ["image"]
            - _target_: monai.transforms.Orientationd
              keys: ["image"]
              axcodes: SPL
            - _target_: monai.transforms.Spacingd
              keys: ["image"]
              pixdim: [1.0, 1.0, 1.0]
              mode: bilinear
            - _target_: monai.transforms.CropForegroundd
              keys: ["image"]
              source_key: "image"
            - _target_: monai.transforms.SpatialPadd
              keys: ["image"]
              spatial_size: "%img_size"
              value: -1024
            - _target_: monai.transforms.ScaleIntensityRanged
              keys: ["image"]
              a_min: -1024
              a_max: 2048
              b_min: 0
              b_max: 1
              clip: True
            - _target_: monai.transforms.RandSpatialCropd
              keys: ["image"]
              roi_size: "%img_size"
            - _target_: torchvision.transforms.Lambda
              lambd: "$lambda x: x['image'].as_tensor()"
            - _target_: project.transforms.dinov2_aug.DINOv2Augmentation3D
              global_view_scale: [0.3, 1.0]
              global_view_size: "$@img_size[0]"
              local_view_scale: [0.3, 1.0]
              local_view_size: "$@img_size[0]//2"
              num_local_views: 0 # No local views for now, we need to add suport for interpolated position embeddings
            - _target_: torchvision.transforms.Lambda
              lambd: "$lambda x: (x, False)" # Incase you need labels.